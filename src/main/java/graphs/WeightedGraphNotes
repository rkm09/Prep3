//------------Notes-------------------//

The Bellman–Ford algorithm is an algorithm that computes the shortest path from a single source vertex to all the other vertices in a weighted digraph.
It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some edge weights are negative numbers.
Like Dijkstra's algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution.

(Dijkstra's algorithm): uses a "Priority Queue" to "greedily" select the closest vertex that has not yet been processed,
and performs this relaxation process on all of its outgoing edges;
by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this {|V|-1} times, where {|V|} is
the number of vertices in the graph.

(Bellman–Ford algorithm): Since the longest possible path without a cycle can be {|V|-1} edges, the edges must be scanned {|V|-1} times to ensure the shortest path has been found for all nodes.
A final scan of all the edges is performed and if any distance is updated, then a path of length {|V|} edges has been found which can only occur if at least one negative cycle exists in the graph.
A common improvement when implementing the algorithm is to return early when an iteration of step 2 fails to relax any edges, which implies all shortest paths have been found, and therefore there are no negative cycles.
The algorithm works by relaxing edges in the graph, meaning that it tries to improve the shortest path estimate for each node in the graph until the solution is found.

The (Shortest Path Faster Algorithm): (SPFA) is an improved version of the Bellman–Ford algorithm that computes single source shortest paths in a weighted directed graph.
The algorithm works well on sparse random graphs, especially those that contain negative-weight edges.

SPFA is similar to the Bellman-Ford algorithm in that each vertex is used as a candidate to relax its adjacent vertices.
However, instead of trying all vertices blindly, SPFA maintains a queue of candidate vertices
and adds a vertex to the queue only if that vertex is relaxed, repeating the process until no more vertices can be relaxed.

The key is maintaining a running maximum probability for each node, and using this maximum to calculate the probabilities for its neighbors.
If the probability of traveling from the starting node to a neighbor node through a specific edge is greater than the current maximum probability for that neighbor, we update the maximum probability of this neighbor node, and add this neighbor node to the queue.
We only update the probability of reaching a neighbor node, say nxt_node and add it back to queue if the current path increases the probability of reaching nxt_node
from the starting node. Moreover, the weight (probability) of each path is less than or equal to 1. Therefore, even if the graph contains a cycle, the product of the probabilities of all edges in the cycle is still less than or equal to 1.
Since loops do not increase the probability of reaching a node, paths that contain loops will be excluded from consideration and not added to the queue.

(Dijkstra's algorithm):
In BFS, we explore the graph in a breadth-first manner, which may not always lead to the shortest path.
This is because BFS does not take into account the weights of the edges and only considers the number of hops.
In contrast, Dijkstra's algorithm takes into account the weights of the edges and always guarantees to find the highest probability
from the source node to any other node in the graph. This is where Dijkstra's algorithm becomes more suitable than BFS, as it takes into account the weights (probabilities) of the edges
and can find the path with the highest probability of reaching the end node.
In order to always select the node with the highest reaching probability, we use a priority queue pq to store the nodes to visit,
where the node with the highest probability of being reached from the starting node has the highest priority.


// ---------------------Notes on Minimum Spanning Tree----------------------//

A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph
that connects all the vertices together, without any cycles and with the minimum possible total edge weight.

Algorithms: Prim's, Kruskal's; both are greedy algorithms.

Kruskal's:
____________
The key steps of the algorithm are sorting and the use of a disjoint-set data structure to detect cycles.
Its running time is dominated by the time to sort all of the graph edges by their weight.

//----notes on DSU----//
Path compression in union find makes it faster:
While obtaining the root, we compress the path by assigning the grandparent of the node as the parent (thereby skipping connections and moving nodes closer to the root).

In an efficient implementation, tree height is controlled using union by size or union by rank.
Both of these require a node to store information besides just its parent pointer. This information is used to decide which root
becomes the new parent. Both strategies ensure that trees do not become too deep.

Union by size:
In the case of union by size, a node stores its size, which is simply its number of descendants (including the node itself).
When the trees with roots x and y are merged, the node with more descendants becomes the parent.

Union by rank:
A node stores its rank, which is an upper bound for its height. When a node is initialized, its rank is set to zero.
To merge trees with roots x and y, first compare their ranks. If the ranks are different, then the larger rank tree becomes the
parent, and the ranks of x and y do not change.

It is clear from the implementations that the size and rank of a node do not matter unless a node is the root of a tree.
Once a node becomes a child, its size and rank are never accessed again.

Every node has rank O(log n) or less. Consequently, each rank can be stored in O(log log n) bits and all the ranks can be stored in O(n log log n) bits.
This makes the ranks an asymptotically negligible portion of the forest's size.

Time Complexity of DSU:
O(M log* N) where log* denotes the iterated logarithm (extremely slow-growing inverse Ackermann function). It is the number of times the logarithm function
must be iteratively applied before the result is less than or equal to 1.


Prim's algorithm:
__________________
It is also a greedy algorithm for building a minimum spanning tree in a weighted and undirected graph.
In this algorithm, we include an arbitrary node in the MST and keep on adding the lowest-weighted edges of the nodes present in the MST
until all nodes are included in the MST and no cycles are formed.

In this algorithm, we can pick any node to start with. Then we will choose the lowest-weighted edge that connects a node present
in the MST to a node not present in the MST.

We could keep all of the edges in an array and then sort them. But then, for each new node that we add to the MST, we would have to add the new node's edges to
the array and sort the array again. This would be a costly operation when done repeatedly.

A more efficient way to track which edges are available and which of these edges has the lowest weight is to use a "min-heap" data structure.
A min-heap is a tree-like data structure that always stores the minimum valued element (edge weight here) at the root
and where insertion and removal of elements (edges) take logarithmic time.

Now, we know how to greedily pick the lowest-weighted edge, but how can we check if including an edge will form a cycle in the MST?
we can use one boolean array to record which nodes are already present in the MST. If both of an edge's nodes are already present in the MST,
we will discard the edge. Otherwise, we will include this edge and mark the newly added node as present in the MST.

Since an MST can only have n - 1 edges, we can use it as an early exit condition to stop iterating over heap elements.



